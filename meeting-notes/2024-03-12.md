---
title: Working Installation of llama2
author: Kat Nykiel, Abdikarim Abdi
date: March 12, 2024
---

## Review from last meeting

- did you have a chance to look through the git guide?
- you have access to the the git repo on GitHub, try making a commit?

## Ollama

### Installing ollama

- using brew, `brew install ollama`
- installation should be very straightforward

### Pulling and running models

- while we wait for supercomputer access, try running on your Mac
- how much RAM does your mac have? Spotlight > About > Memory
- try `ollama run llama2`, this will let you run meta's open model [llama2](https://llama.meta.com/llama2/)

### Intro to prompt engineering

- Prompt engineering vs. fine-tuning: not adjusting the parameters of the network, but adding more information as an input to the network
- We are just doing *prompt engineering*, but fine-tuning will be useful when are working with massive amounts of data and have access to GPUs

Taken [directly from the README](https://github.com/ollama/ollama?tab=readme-ov-file#customize-a-prompt):

Models from the Ollama library can be customized with a prompt. For example, to customize the llama2 model:

```
ollama pull llama2
```

Create a Modelfile:

```
FROM llama2

# set the temperature to 1 [higher is more creative, lower is more coherent]
PARAMETER temperature 1

# set the system message
SYSTEM """
You are Mario from Super Mario Bros. Answer as Mario, the assistant, only.
"""
```
Next, create and run the model:

```
ollama create mario -f ./Modelfile
ollama run mario
>>>hi
Hello! It's your friend Mario.
```

## Objective for next meeting

- Find a news article of your choice
- Extract the text content from the article
- Create a system prompt and a new model for this article
- Create some queries with this model to understand how much it "knows" about the article
- Share what you find in this repo on github (.md, .tex, .pdf, .ipynb, etc.)

If you have more time,

- think about how you would go about automating this process with python