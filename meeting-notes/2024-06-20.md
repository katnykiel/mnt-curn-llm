---
title: Workflow, F-Score Update
author: Kat Nykiel, Bryan Pintado
date: June 20, 2024
---

## Notes

- is `export-workflow.py` working for you?
  - github repo updated on Bryan's computer
  - have you tweaked around with the prompts at all
- f-score
  - question about how we can make the data numeric
  - f-score uses binary classification, we need to project our data onto this space
    - true positive: human/llm both find expert
    - false positive: llm finds expert human doesn't
    - false negative: human finds expert llm doesn't
    - true negative: human/llm both find no expert (doesn't contribute to f1 score)
  - convert to a numeric space by matching strings as booleans

### Sample Implementation

```python

true_positive = 0
false_positive = 0
false_negative = 0

llm_experts = []
human_experts = []

for llm in llm_experts:
    for human in human_experts:
        if llm == human:
            true_positive += 1

        if llm not in human_experts:
            false_positive += 1

        if human not in llm_experts:
            false_negative += 1

```

## Goals

- get the expert-workflow code running
  - using llama2 instead of llama3 for now
  - connect to the list of URLs generated from the API
  - run on the sample of articles that we labeled by hand
  - tweak the prompt to 
- post-process the LLM output to get a list of experts found
  - get a list of strings of the authors for each article
    - identified from the LLM
    - manually identified
  - append the following to the expert workflow file
```python
    import re
    experts = re.findall(r'<expert>(.*?)</expert>', response["answer"]) 
```
- calculating f1-score with string matching as booleans
  - for now, only report the f1 score of author identification, not institutions