---
title: Using Langchain to Extract Experts
author: Kat Nykiel, Abdikarim Abdi
date: April 11, 2024
---

## Review from Last Meeting 

- Working through the LangChain quickstart
- Uploading the `Article` class to GitHub

## Adding a Class Method to `Article` for LangChain

### Accessing LLMs

```python
from langchain_community.llms import Ollama
llm = Ollama(model="llama2")
```

### Using `llm.invoke`

```python
llm.invoke("how can langsmith help with testing?")
```

### Prompt Templates

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""")

document_chain = create_stuff_documents_chain(llm, prompt)
```

### Invoke Our Document Chain

```python
from langchain_core.documents import Document

document_chain.invoke({
    "input": "what experts are referenced in the following article?",
    "context": [Document(page_content="[[[ add article content here ]]]")]
})

```

## Goals

- Adding a class method to `Article` for LangChain to get expert references
    - Alter the prompt to get responses in a structured format that we can easily add to our `Article` object
    - JSON may be the easiest strategy
- Document how well the LLMs are at this task
    - which LLMs seem to be most accurate? (you could even do a for loop and try many different models)
    - what are the common mistakes these LLMs make?