---
title: Multi-model LLMs
author: Kat Nykiel, Abdikarim Abdi
date: March 14, 2024
---

## Review from last meeting

- what were you able to find the LLMs were good at querying?
- writing a document on it / adding to GitHub
- what steps in the process did you find challenging?

## Multi-modal LLMs

### Moving this repository to your own machine

- install git: `brew install git`
- move to desired location with `cd`
- download the repo: `git clone https://github.com/katnykiel/mnt-curn-llm.git`

### Using ollama with files

- summarizing the content of meeting notes
- make a terminal at the directory where these notes are stored
- run `ollama run llama2 "Summarize this file: $(cat meeting-notes/2024-03-12.md)"`

### Using ollama with images

- Instead of just passing text to our LLM as an input, we can pass images
- with ollama, this is done with file paths 
  - `ollama run llama2`
  - `>>> What's in this image? /path/to/file`
- this is tricky, may not work on your machine

## Objective for next meeting

- find a news article that cites an expert
- copy the news article into a .md file
- create a system prompt that is good at picking out an expert in a news article
- run the LLM with this file and document what you get as a response

This is the basis for the routine we'll use to perform high-throughput analysis of news articles.
