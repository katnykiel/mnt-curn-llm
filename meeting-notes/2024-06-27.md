---
title: Checking F-Score for Various Prompts
author: Kat Nykiel, Bryan Pintado
date: June 27, 2024
---

## Notes

- f-score implementation status?
- nanoHUB workflow isn't an option, we can't run ollama there
  - we could submit jobs to Purdue's supercomputers through nanoHUB
  - @kat: set up nanoHUB intro, figure out if we can request GPUs through nanohub
  - alternatively, Kat can run them on Purdue's supercomputers without much hassle
- given a list of article URLs, it's near trivial to swap out LLMs/prompts and evaluate the f-score of each

## Goals

- finish f-score implementation
  - extract the lists of lists for human-label and llm-label
  - implement f-score using procedure from [[2024-06-20]]
  - calculate f-score for a single article
  - report an average score across the articles
- create a write-up of different prompts and their f-scores

```python
urls = [...]

df = pd.read_excel("...")

expertss = []
for url in urls:
  sub_df = df[df['url'] == url]
  experts = df['experts'].values()
  expertss.append(experts)
```
